## Evals

Test your prompts. Can handle multiple models, prompts and variables at the same time.

Uses [ollama](https://github.com/ollama/ollama-python) to run LLMs locally.

## How to use

1. Set the models, prompts and variables.
1. Run `python eval.py`.
1. Logs from the run will be in `output/<datetime>.yaml`.

## TODO

-   fix grading
-   add tests
-   add [asyncio](https://github.com/ollama/ollama-python?tab=readme-ov-file#async-client)
-   fix formatting

## Resources

-   https://arize.com/blog-course/evaluating-prompt-playground/
-   https://www.confident-ai.com/blog/a-gentle-introduction-to-llm-evaluation
-   https://stephencollins.tech/posts/how-to-use-promptfoo-for-llm-testing
-   https://github.com/hegelai/prompttools
-   https://github.com/microsoft/promptbench
-   https://github.com/promptfoo/promptfoo
-   https://github.com/langfuse/langfuse
-   https://promptmetheus.com/
-   https://openshiro.com/
-   https://promptknit.com/
-   https://learnprompting.org/docs/tooling/IDEs/intro
-   https://www.promptotype.io/
-   https://langbear.runbear.io/introduction
