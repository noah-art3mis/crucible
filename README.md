## Evals

Test your prompts.

https://github.com/ollama/ollama
https://github.com/ollama/ollama-python
https://github.com/ollama/ollama/blob/main/docs/api.md

## TODO

-   add [asyncio](https://github.com/ollama/ollama-python?tab=readme-ov-file#async-client)

## Resources

-   https://github.com/anthropics/anthropic-cookbook/blob/main/misc/building_evals.ipynb
-   https://arize.com/blog-course/evaluating-prompt-playground/
-   https://www.confident-ai.com/blog/a-gentle-introduction-to-llm-evaluation
-   https://stephencollins.tech/posts/how-to-use-promptfoo-for-llm-testing
-   https://github.com/hegelai/prompttools
-   https://github.com/microsoft/promptbench
-   https://github.com/promptfoo/promptfoo
-   https://github.com/langfuse/langfuse
-   https://promptmetheus.com/
-   https://openshiro.com/
-   https://promptknit.com/
-   https://learnprompting.org/docs/tooling/IDEs/intro
-   https://www.promptotype.io/
-   https://langbear.runbear.io/introduction
